{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger tous les  fichiers et en créer une liste de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data_path\"_README.txt\"\n",
    "files = [f for f in sorted(os.listdir(data_path)) if not f.startswith('.') and f.endswith('.txt')]\n",
    "len(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for f in files:\n",
    "    try:\n",
    "        with open(os.path.join(data_path, f), \"r\", encoding=\"utf-8\") as file:\n",
    "            texts.append(file.read())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier {f} n'a pas été trouvé dans le chemin {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 1\n",
    "tfidf_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer un algorithme de clustering sur les vecteurs TF-IDF des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour en savoir plus sur le KMeans clustering :\n",
    "- https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduire les vecteurs à 2 dimensions à l'aide de l'algorithme PCA\n",
    "Cette étape est nécessaire afin de visualiser les documents dans un espace 2D\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des listes pour chaque cluster\n",
    "cluster_contents = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Parcourir chaque cluster et ajouter le contenu des fichiers à la liste correspondante\n",
    "for cluster, filenames in clustering.items():\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            with open(os.path.join(data_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                cluster_contents[cluster].append(file.read())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Erreur : Le fichier {filename} n'a pas été trouvé dans le chemin {data_path}\")\n",
    "\n",
    "# Vous avez maintenant trois listes : cluster_contents[0], cluster_contents[1], cluster_contents[2]\n",
    "# Chaque liste contient le contenu des documents de son cluster respectif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1er cluster (cluster_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"sach\", \"heures\", \"toutes\", \"hier\", \"très\"\n",
    "       , \"etc\", \"leurs\", \"grandes\", \"lés\", \"ille\", \"quo\", \"peu\", \"bon\", \"vers\", \"grand\", \"puis\", \"mois\", \"personne\",\n",
    "       \"devant\", \"beau\", \"mén\", \"elles\", \"toujours\", \"déjà\", \"avenue\", \"quatre\", \"fort\", \"jours\", \"aujourd\",\n",
    "       \"car\", \"hier\", \"toute\", \"grands\", \"app\", \"matin\", \"bne\", \"bas\", \"adresser\", \"haut\", \"dés\",\n",
    "       \"place\", \"rue\", \"Bruxelles\", \"DCM\", \"ecr\", \"jour\",\"maison\", \"ans\", \"chez\", \"réf\", \"prendre\",\n",
    "       \"mod\", \"pers\", \"suite\", \"Mme\", \"voir\", \"cause\", \"louer\", \"porte\", \"belle\", \"fille\",\n",
    "       \"adr\", \"quart\", \"maisons\", \"rué\", \"fit\", \"seule\", \"agence\", \"plusieurs\", \"bonnes\", \"ceux\", \"quelques\",\n",
    "       \"faits\", \"jeune\", \"cent\", \"vient\", \"point\", \"portées\", \"avant\", \"serv\", \"com\", \"jard\",\n",
    "       \"enfant\", \"désire\", \"cuis\", \"Bruxelles\", \"bruxelles\", \"grande\", \"petit\", \"bons\", \"dcm\", \"soir\",\n",
    "       \"près\", \"bonne\", \"demande\", \"prix\", \"fois\", \"dés\", \"vendre\", \"jamais\", \"chaussée\", \"bonne\", \"franc\", \"lieu\", \"rien\", \"quartier\",\n",
    "       \"pris\", \"fois\", \"tranq\", \"pos\", \"vente\", \"bonne\", \"neuve\", \"cuisine\", \"servante\", \"mat\", \"écrire\",\n",
    "       \"juillet\", \"notaire\", \"dès\", \"quand\", \"temps\", \"pet\", \"celui\", \"donné\", \"partie\", \"homme\", \"petite\", \"cours\",\n",
    "       \"mme\",\"part\", \"coup\", \"demi\", \"pens\", \"neuf\", \"ruo\", \"cinq\", \"ici\", \"cond\", \"ferme\", \"coucher\", \"demandé\",\n",
    "       \"ferm\", \"comm\", \"notaires\", \"août\", \"prop\", \"alors\", \"mardi\", \"beaucoup\", \"enfants\", \"ventes\", \"lundi\", \"jeudi\",\n",
    "       \"donner\", \"nuit\", \"chamb\", \"chaque\", \"dimanche\", \"dos\", \"uno\", \"garn\", \"lés\", \"fam\", \"jolie\", \"demain\", \"année\",\n",
    "       \"petite\", \"fam\", \"uno\", \"celui\", \"contenant\", \"premier\", \"assez\", \"nouvelles\", \"beaucoup\", \"cond\", \"con\",\n",
    "       \"PET\", \"vendredi\", \"quand\", \"nouveau\", \"gros\", \"dix\", \"mieux\", \"journ\", \"chaque\", \"cert\", \"dém\", \"dos\",\n",
    "       \"vend\", \"chamb\", \"alors\", \"seulement\", \"mars\", \"delà\", \"trop\", \"courant\", \"demain\", \"brux\",\n",
    "       \"dernier\", \"hon\", \"bel\", \"seul\", \"frais\", \"petits\", \"dernière\", \"mis\", \"diverses\", \"référ\",\n",
    "       \"jne\", \"tant\", \"franco\", \"francs\",\"quelque\", \"nouvelle\", \"vieux\", \"placé\", \"garni\",\n",
    "       \"mal\", \"sachant\", \"six\", \"nommé\", \"propre\", \"avril\", \"coud\", \"juin\", \"octobre\", \"enf\",\n",
    "       \"mis\", \"divers\", \"sait\", \"servi\", \"flam\", \"septembre\", \"frais\", \"hom\", \"bel\", \"hon\",\n",
    "       \"ouvrir\", \"enfin\", \"février\", \"hui\", \"dessus\", \"lès\", \"janvier\", \"scs\", \"partout\", \"façon\",\n",
    "       \"laquelle\", \"ste\", \"samedi\", \"mans\", \"années\", \"ment\", \"mai\", \"voici\", \"céder\", \"surtout\",\n",
    "       \"mercredi\", \"parmi\", \"franç\", \"font\", \"ancien\", \"cond\", \"mlle\", \"jeunes\", \"beaux\", \"semaine\",\n",
    "       \"garnie\", \"reçu\", \"trouve\", \"derrière\", \"ruc\", \"rest\", \"conn\", \"lit\", \"haute\", \"parce\", \"fin\",\n",
    "       \"aucune\", \"aussitôt\", \"ouvr\", \"aucune\", \"donne\", \"certains\", \"ord\", \"peuvent\", \"loin\", \"décembre\",\n",
    "       \"autant\", \"possible\", \"première\", \"prochains\", \"faite\", \"voilà\", \"rendre\", \"maintenant\", \"ensuite\",\n",
    "       \"los\", \"novembre\", \"presque\", \"ailleurs\", \"longtemps\", \"veut\", \"heure\", \"cependant\", \"écrit\", \"pourrait\",\n",
    "        \"comment\", \"mettre\", \"abord\", \"oui\", \"mêmes\", \"nombreux\", \"également\", \"malgré\" ]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = 'data/tmp/'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'cluster_0.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(cluster_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"cluster_0.txt\"\n",
    "        output_path = f\"cluster_0_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}cluster_0.txt\"\n",
    "        output_path = f\"{folder}/cluster_0_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp_path, f'cluster_0_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "\n",
    "cloud.to_file(os.path.join(temp_path, f\"*.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"*.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
