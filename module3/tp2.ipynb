{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de mots-clefs avec Yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un nouveau repo consacré aux fichiers séléctionnés de l'année 1893\n",
    "!mkdir ../data/txt/txt_1893\n",
    "!cp ../data/txt/KB_JB838_1893-*.txt ../data/txt/txt_1893\n",
    "!wc -l ../data/txt/txt_1893/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/txt/txt_1893\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir un fichier\n",
    "this_file = files[0]\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer le texte du fichier\n",
    "text = open(os.path.join(data_path, this_file), 'r', encoding='utf-8').read()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Réaliser la même opérations sur tous les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(files)[:100]:\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"sach\", \"heures\", \"toutes\", \"hier\", \"très\"\n",
    "       , \"etc\", \"leurs\", \"grandes\", \"lés\", \"ille\", \"quo\", \"peu\", \"bon\", \"vers\", \"grand\", \"puis\", \"mois\", \"personne\",\n",
    "       \"devant\", \"beau\", \"mén\", \"elles\", \"toujours\", \"déjà\", \"avenue\", \"quatre\", \"fort\", \"jours\", \"aujourd\",\n",
    "       \"car\", \"hier\", \"toute\", \"grands\", \"app\", \"matin\", \"bne\", \"bas\", \"adresser\", \"haut\", \"dés\",\n",
    "       \"place\", \"rue\", \"Bruxelles\", \"DCM\", \"ecr\", \"jour\",\"maison\", \"ans\", \"chez\", \"réf\", \"prendre\",\n",
    "       \"mod\", \"pers\", \"suite\", \"Mme\", \"voir\", \"cause\", \"louer\", \"porte\", \"belle\", \"fille\",\n",
    "       \"adr\", \"quart\", \"maisons\", \"rué\", \"fit\", \"seule\", \"agence\", \"plusieurs\", \"bonnes\", \"ceux\", \"quelques\",\n",
    "       \"faits\", \"jeune\", \"cent\", \"vient\", \"point\", \"portées\", \"avant\", \"serv\", \"com\", \"jard\",\n",
    "       \"enfant\", \"désire\", \"cuis\", \"Bruxelles\", \"bruxelles\", \"grande\", \"petit\", \"bons\", \"dcm\", \"soir\",\n",
    "       \"près\", \"bonne\", \"demande\", \"prix\", \"fois\", \"dés\", \"vendre\", \"jamais\", \"chaussée\", \"bonne\", \"franc\", \"lieu\", \"rien\", \"quartier\",\n",
    "       \"pris\", \"fois\", \"tranq\", \"pos\", \"vente\", \"bonne\", \"neuve\", \"cuisine\", \"servante\", \"mat\", \"écrire\",\n",
    "       \"juillet\", \"notaire\", \"dès\", \"quand\", \"temps\", \"pet\", \"celui\", \"donné\", \"partie\", \"homme\", \"petite\", \"cours\",\n",
    "       \"mme\",\"part\", \"coup\", \"demi\", \"pens\", \"neuf\", \"ruo\", \"cinq\", \"ici\", \"cond\", \"ferme\", \"coucher\", \"demandé\",\n",
    "       \"ferm\", \"comm\", \"notaires\", \"août\", \"prop\", \"alors\", \"mardi\", \"beaucoup\", \"enfants\", \"ventes\", \"lundi\", \"jeudi\",\n",
    "       \"donner\", \"nuit\", \"chamb\", \"chaque\", \"dimanche\", \"dos\", \"uno\", \"garn\", \"lés\", \"fam\", \"jolie\", \"demain\", \"année\",\n",
    "       \"petite\", \"fam\", \"uno\", \"celui\", \"contenant\", \"premier\", \"assez\", \"nouvelles\", \"beaucoup\", \"cond\", \"con\",\n",
    "       \"PET\", \"vendredi\", \"quand\", \"nouveau\", \"gros\", \"dix\", \"mieux\", \"journ\", \"chaque\", \"cert\", \"dém\", \"dos\",\n",
    "       \"vend\", \"chamb\", \"alors\", \"seulement\", \"mars\", \"delà\", \"trop\", \"courant\", \"demain\", \"brux\",\n",
    "       \"dernier\", \"hon\", \"bel\", \"seul\", \"frais\", \"petits\", \"dernière\", \"mis\", \"diverses\", \"référ\",\n",
    "       \"jne\", \"tant\", \"franco\", \"francs\",\"quelque\", \"nouvelle\", \"vieux\", \"placé\", \"garni\",\n",
    "       \"mal\", \"sachant\", \"six\", \"nommé\", \"propre\", \"avril\", \"coud\", \"juin\", \"octobre\", \"enf\",\n",
    "       \"mis\", \"divers\", \"sait\", \"servi\", \"flam\", \"septembre\", \"frais\", \"hom\", \"bel\", \"hon\",\n",
    "       \"ouvrir\", \"enfin\", \"février\", \"hui\", \"dessus\", \"lès\", \"janvier\", \"scs\", \"partout\", \"façon\",\n",
    "       \"laquelle\", \"ste\", \"samedi\", \"mans\", \"années\", \"ment\", \"mai\", \"voici\", \"céder\", \"surtout\",\n",
    "       \"mercredi\", \"parmi\", \"franç\", \"font\", \"ancien\", \"cond\", \"mlle\", \"jeunes\", \"beaux\", \"semaine\",\n",
    "       \"garnie\", \"reçu\", \"trouve\", \"derrière\", \"ruc\", \"rest\", \"conn\", \"lit\", \"haute\", \"parce\", \"fin\",\n",
    "       \"aucune\", \"aussitôt\", \"ouvr\", \"aucune\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une année\n",
    "year = 1893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers de cette année\n",
    "data_path = '../data'\n",
    "txt_path = '../data/txt'\n",
    "txts = [f for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f)) and str(year) in f]\n",
    "len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le contenu de ces fichiers dans une liste\n",
    "content_list = []\n",
    "for txt in txts:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding='utf-8') as f:\n",
    "        content_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'éléments (=fichiers) dans la liste\n",
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de nettoyage\n",
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(year, folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afficher les termes les + fréquents\n",
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Créer; afficher et stocker le wordcloud\n",
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnaissance d'entités nommées avec SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/tmp/1893.txt\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "location = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "        location[ent.text] += 1\n",
    "        sorted_location = sorted(location.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for location, freq in sorted_location[:50]:\n",
    "    print(f\"{location} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "org = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "        org[ent.text] += 1\n",
    "        sorted_org = sorted(org.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for org, freq in sorted_org[:50]:\n",
    "    print(f\"{org} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\", use_pt=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "\n",
    "sentiment_analyser = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Les funérailles du maire Alexiéw, aux frais de la ville, ont été graiidioses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Les funérailles du maire Alexiéw, aux frais de la ville, ont été grandioses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Paradoxes et vérités': . . Trop d’esprit humilie ceux qui n’en ont pas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"n homme a ouvert la porte, un mulâtre de taille assez corpulente, aux cheveux déjà couleur de neige, aux yeux jaunes, aux dents blanches qui font paraître sa figura basanéo encore plus noire. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Èt ainsi pendant des années on « étudiera » lé redressement de la Montagne de la Cour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\" L’histoire de son «sauvetage», qui ressemblait fort à fine capture, est encore dans tôutès les mémoires et l’on n’a pas oublié l’accident tragique »— une chute parla fenêtre — qui, à Bagamoyô, au moment où on le rendait de force aux délices de la civilisation, faillit coûter la vie à celui que. les fatigues, les privations, les maladies, la rébellion même avaient épargné : comme si l’Afrique, à laquelle on l’arrachait, voulait le faire souvenir de çe nom (le « Fidèle « qu’ii avait pris en venant vivre sur son sol. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Des personnes, dont les préoccupations patriotiques exagèrent peut-être les craintes, ariirment que l’outillage de celto. manufacture est défectueux.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Les', grévistes.' se sont réfugiés dans les cours et dansjes jardins ét de là ont fait pleuvoir sur les gendarmes ùnegrèle de pierres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Les mesures prises par le gouvernement pour arrêter le mouvement d’extension de la grève vont être renforcées par l’envoi, demain lundi, de nouveaux régiments dans lé Hainaut et à Garni.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Il parait que la garnison de Gand sera renforcée par de la cavalerie, un régiment entier d’infanterie et même du génie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"On continue à amener à la prison de Mous les grévistes arrêtés ,pour atteinte à la liberté du travail et pour coups aux gendarmes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
